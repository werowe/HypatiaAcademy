{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bd512c6-99dc-4da9-a73e-97bcd508a031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-07 15:05:22--  https://github.com/werowe/HypatiaAcademy/raw/refs/heads/master/ml/my_array.npy\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/werowe/HypatiaAcademy/refs/heads/master/ml/my_array.npy [following]\n",
      "--2025-09-07 15:05:22--  https://raw.githubusercontent.com/werowe/HypatiaAcademy/refs/heads/master/ml/my_array.npy\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44528 (43K) [application/octet-stream]\n",
      "Saving to: ‘my_array.npy’\n",
      "\n",
      "my_array.npy        100%[===================>]  43.48K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-09-07 15:05:23 (1.03 MB/s) - ‘my_array.npy’ saved [44528/44528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/werowe/HypatiaAcademy/raw/refs/heads/master/ml/my_array.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d28ad42-156c-4304-b7f6-bc8a0185f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X=np.load('my_array.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c60da-83c4-40d7-b148-e1eb00746776",
   "metadata": {},
   "source": [
    "# How to Compute Attention Over Bi-LSTM Hidden States\n",
    "\n",
    "## Steps for Computing Attention\n",
    "\n",
    "### 1. Gather Bi-LSTM Hidden States\n",
    "\n",
    "Stack the Bi-LSTM hidden states for all input timesteps:  \n",
    "$ a^{\\langle 1 \\rangle}, a^{\\langle 2 \\rangle}, \\ldots, a^{\\langle T_x \\rangle} $  \n",
    "These are typically concatenated forward and backward states per timestep.\n",
    "\n",
    "### 2. Use Decoder State\n",
    "\n",
    "Take the current decoder hidden state:  \n",
    "$ s^{\\langle t-1 \\rangle} $ \n",
    "This is the state computed by the decoder at the previous output step.\n",
    "\n",
    "### 3. Combine for “Energy” Scores\n",
    "\n",
    "For each encoder timestep $ i $, concatenate $ a^{\\langle i \\rangle} $ and $ s^{\\langle t-1 \\rangle} $.  \n",
    "Feed this combined vector into a small neural network (“energy” function), typically a dense layer with tanh/relu activation:\n",
    "\n",
    "$\n",
    "e_i = f_{\\text{energy}}(a^{\\langle i \\rangle}, s^{\\langle t-1 \\rangle})\n",
    "$\n",
    "\n",
    "This yields a scalar energy score for each encoder position.\n",
    "\n",
    "### 4. Compute Attention Weights\n",
    "\n",
    "Collect all $ e_i $ and apply softmax across all encoder positions to get attention weights:\n",
    "\n",
    "$\n",
    "\\alpha_i = \\frac{\\exp(e_i)}{\\sum_{j=1}^{T_x} \\exp(e_j)}\n",
    "$\n",
    "\n",
    "### 5. Calculate Context Vector\n",
    "\n",
    "Use the attention weights to compute a weighted sum over the Bi-LSTM hidden states:\n",
    "\n",
    "$\n",
    "\\text{context}^{\\langle t \\rangle} = \\sum_{i=1}^{T_x} \\alpha_i \\cdot a^{\\langle i \\rangle}\n",
    "$\n",
    "\n",
    "This context vector is fed into the decoder LSTM for generating output at time \\( t \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Illustrated Flow\n",
    "\n",
    "| Step             | Inputs                       | Operation                   | Output             |\n",
    "|------------------|-----------------------------|-----------------------------|--------------------|\n",
    "| Energy scoring   | $(a^{\\langle i \\rangle}, s^{\\langle t-1 \\rangle}$) & | Dense network               | $e_i$ per position |\n",
    "| Softmax          | $[e_1, ..., e_{T_x}]$     | Normalization               | $[α_1, ..., α_{T_x}]$ |\n",
    "| Weighted sum     | $(a^{\\langle i \\rangle}, \\alpha_i)$ | Dot product       | context vector     |\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- The process is identical for Bi-LSTM as for standard LSTM, except \\( a \\) is dimensionality-doubled due to concatenation of forward and backward states.\n",
    "- This setup allows the decoder to dynamically \"attend\" to different parts of the input at every output step, based on both encoder features and current decoding state.\n",
    "\n",
    "This is the standard attention mechanism used in classic sequence-to-sequence models with Bi-LSTM encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "574e0f52-9475-449e-afa9-067ee5eebd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30, 37)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b5793-a36e-4614-8f98-7dd4b9c01357",
   "metadata": {},
   "source": [
    "`model = tf.keras.Model(inputs, forward_seq, backward_seq, h_fw, h_bw)`\n",
    "\n",
    "Defines a Keras model whose outputs are:\n",
    "\n",
    "1.\tThe time series of forward hidden states (all timesteps)\n",
    "2.\tThe time series of backward hidden states (all timesteps)\n",
    "3.\tThe final forward hidden state (from the last timestep of the forward LSTM)\n",
    "4.\tThe final backward hidden state (from the last timestep of the backward LSTM)\n",
    "\n",
    "\n",
    "outputs: sequence, h_fw, c_fw, h_bw, c_bw\n",
    "\n",
    "* h_fw: The final hidden state (output at the last timestep) from the forward LSTM. Shape: (batch_size, units) = (5, 16)\n",
    "  \n",
    "* c_fw: The final cell state from the forward LSTM.\n",
    "\t \n",
    "* h_bw: The final hidden state from the backward LSTM.\n",
    " \n",
    "* c_bw: The final cell state from the backward LSTM.\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "948e8b2c-f4c8-4750-a7bf-8860d873be8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 30, 32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cdffb7-fcad-46af-ac6b-5807ac1d0483",
   "metadata": {},
   "source": [
    "2. **Typical process per decoder step:**\n",
    "   - For each position $i$, **concatenate** the encoder hidden state at $i$ ($a^{\\langle i \\rangle}$, from `sequence[:, i, :]`) with the decoder’s current hidden state ($s_{t-1}$).\n",
    "   - Pass each concatenated vector through a small dense network (often just a Dense layer with tanh, sometimes more).\n",
    "   - **Collect all the energy scores** for each position into a vector, then apply softmax over encoder positions (across timesteps) to get attention weights ($\\alpha_i$).\n",
    "   - Compute the **context vector** as the weighted sum of encoder hidden states, weighted by $\\alpha_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e4467-cb5a-4b4a-9c8a-354d809a28bb",
   "metadata": {},
   "source": [
    "The variable `s_prev`—the decoder's previous hidden state—**does not come from your Bi-LSTM encoder directly** during attention computation at each step. Here's how it's used and initialized in encoder-decoder (seq2seq) architectures:\n",
    "\n",
    "## Where Does `s_prev` Come From?\n",
    "\n",
    "- **At the first decoder step (t=1):**\n",
    "  - The decoder's initial hidden state (`s_prev`) is typically set using the **final hidden states of the encoder**.\n",
    "  - With a Bi-LSTM encoder, this is often done by **concatenating or transforming** the final forward and backward hidden states (e.g., from `h_fw` and `h_bw`).\n",
    "  - If your decoder’s hidden state dimension is not the same as the concatenated encoder states, use a Dense (fully connected) layer to project them to the correct dimension.\n",
    "\n",
    "- **For subsequent decoding steps (t > 1):**\n",
    "  - The decoder’s current hidden state (`s_t`) is passed as `s_prev` into the next step—it is updated internally by the decoder LSTM based on its previous output and the context vector from attention.\n",
    "\n",
    "## Typical Initialization Summary\n",
    "- **s_prev at t=1:**  \n",
    "  Concatenate `h_fw` and `h_bw` (shape `(batch, 32)`), then possibly pass through a Dense layer to match decoder hidden size (shape `(batch, n_s)`).\n",
    "- **s_prev at t > 1:**  \n",
    "  Output of previous step’s decoder hidden state.\n",
    "\n",
    "## Example for First Step:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b89e50ba-4d88-447f-b9f0-438f077a4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Concatenate, Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# 1. Define the input tensor for the model: shape (sequence_length, feature_dim)\n",
    "inputs = Input(shape=(30, 37))  # (batch_size, 30, 37)\n",
    "\n",
    "# 2. Create a bidirectional LSTM layer.\n",
    "#    - `units=16` means each LSTM (forward and backward) has 16 hidden units.\n",
    "#    - `return_sequences=True` so we get output at every timestep (not just last).\n",
    "#    - `return_state=True` so we get the final h, c for both directions.\n",
    "units = 16\n",
    "bilstm = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    ")\n",
    "\n",
    "# 3. Pass the inputs through the Bi-LSTM.\n",
    "#    - 'sequence' is the full output (batch, time, features).\n",
    "#    - h_fw, c_fw: final hidden + cell for the forward LSTM.\n",
    "#    - h_bw, c_bw: final hidden + cell for the backward LSTM.\n",
    "sequence, h_fw, c_fw, h_bw, c_bw = bilstm(inputs)\n",
    "fw_last = h_fw  # Forward direction last output (batch, units)\n",
    "bw_last = h_bw  # Backward direction last output (batch, units)\n",
    "\n",
    "# 4. Prepare initial decoder hidden state for attention/decoding.\n",
    "#    - Here we concatenate both last forward and backward states.\n",
    "#    - Shape: (batch, 32)\n",
    "s_prev = Concatenate()([fw_last, bw_last])\n",
    "\n",
    "# 5. Define a manual attention layer as a custom Keras Layer.\n",
    "class ManualAttention(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Concatenation layer to combine encoder hidden state and decoder state\n",
    "        self.concat = Concatenate(axis=-1)\n",
    "        # Dense layer to score (energy) each encoder timestep with respect to decoder state\n",
    "        self.energy_fc = Dense(1, activation='tanh')\n",
    "        self.units = units\n",
    "\n",
    "    def call(self, sequence, s_prev):\n",
    "        # sequence: shape (batch, timesteps, 2*units)\n",
    "        # s_prev: shape (batch, decoder_state_dim)\n",
    "        energies = []\n",
    "        # Iterate over each encoder timestep to compute attention \"energy\"\n",
    "        for i in range(sequence.shape[1]):  # For every time step\n",
    "            a_i = sequence[:, i, :]            # Get encoder state at timestep i\n",
    "            concat = self.concat([a_i, s_prev])# Concatenate with decoder state\n",
    "            e_i = self.energy_fc(concat)       # Pass through energy scoring dense layer\n",
    "            energies.append(e_i)               # Collect score for this timestep\n",
    "        # energies: list of (batch,1) => stack into (batch, timesteps, 1)\n",
    "        energies = tf.stack(energies, axis=1)  \n",
    "        energies = tf.squeeze(energies, axis=-1) # (batch, timesteps)\n",
    "        # Apply softmax over all encoder timesteps to get attention weights\n",
    "        alphas = tf.nn.softmax(energies, axis=1) \n",
    "        alphas_expanded = tf.expand_dims(alphas, axis=-1) # (batch, timesteps, 1)\n",
    "        # Compute the context vector as weighted sum of encoder states\n",
    "        context = tf.reduce_sum(alphas_expanded * sequence, axis=1)  # (batch, 2*units)\n",
    "        return context, alphas        # context: summary vector; alphas: attention weights\n",
    "\n",
    "# 6. Create an instance of the ManualAttention layer\n",
    "#    - Here, units*2 since encoder output per timestep is concatenated (fw+bw)\n",
    "manual_attention = ManualAttention(units*2)\n",
    "\n",
    "# 7. Use the ManualAttention layer to compute context and attention weights.\n",
    "#    - context: weighted sum of encoder outputs\n",
    "#    - alphas: attention weights (can be interpreted as where the model \"looked\")\n",
    "context, alphas = manual_attention(sequence, s_prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c967a6-f70c-48d0-9c6d-23b25bd8f1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
