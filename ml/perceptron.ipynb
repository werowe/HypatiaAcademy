{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGrWV4D8qrcbRy8z6PwGvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron\n",
        "\n",
        "This code and the graphics are copied from https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf. by Arthur Arnx writing in Towards Data Science\n",
        "\n",
        "Each circle is a **neuron**.  Read from left to right we have the input layer, which is the training data.  The middle layers are the **hidden layers**.  The rightmost layer is the **output** layer. The output are scalara or probabilities. In this case here are two.  This network would work for a binary classification problem.  An example of that is what we describe below.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*v1ohAG82xmU6WGsG2hoE8g.png)\n",
        "\n",
        "\n",
        "#The XOR Problem\n",
        "The XOR (**Exclusive Or**) problem is a class problem in neural networks.  It shows why the activation function cannot be linear. Because if it was the network would only work with problems with linear solutions.\n",
        "\n",
        "XOR is the exclusive OR.  That means A XOR B is true when A or B is true and not both.  \n",
        "\n",
        "We have four possibilities, where 1 is true and 0 is false.\n",
        "\n",
        "| A      | B      | XOR |\n",
        "| ------------- | ------------- |  ------------- |\n",
        "| 1| 1 | 0 |\n",
        "| 0| 1| 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 0| 0 | 1 |\n",
        "\n",
        "\n",
        "# Neuron\n",
        "You can think of the neural network of the solution to the Y = W X + b problem where W are the coefficient in the model.  Except W in this model, unlike linear regression, are functions.  It is easiest to this of them as compound functions like g(f(x)) where each hidden layer is one more level in this nesting.\n",
        "\n",
        "It solves the network by muliplying the inputs x1, x2, x3 by the weights w1 and w2 and w3.  w1 and w2 are bias and we is weight.m x1 and x2 are inputs are x3 is observed output (i.e., y)\n",
        "\n",
        " Initially w1 , w2, w3 are just guesses.  This process repeats itself (called **backprogatopm** by adjusting the weights until they reach their minimum value.  This is usally dones by taking a partial derivative and looking for where the derivative is 0 or closest to zero, which is the **stochastic gradient solution** technique.  Each pass through the network moves the value of the weight in the opposite direction of the gradient.\n",
        "\n",
        "$$\\begin{bmatrix} 1 & 1 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix} = 1 \\cdot w_1 + 1 \\cdot w_2 + 0 \\cdot w_3 = w_1 + w_2 $$\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1302/format:webp/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg)\n",
        "\n",
        "#Activation Function\n",
        "This is a function that takes the value above and turns it into a number between 0 and 1.  \n",
        "\n",
        "Frabcois Chollet, wrote wrote Keras, says, \"It introduces non-linearity into the model, allowing it to learn complex patterns. Common activation functions include ReLU, sigmoid, and softmax.\"\n",
        "\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHWL_71qml0kP_Imyx4zBg.png)\n",
        "\n",
        "#Loss Function\n",
        "\n",
        "Frabcois Chollet:  A **loss function** is a mathematical function used in machine learning to measure the difference between the predicted output of a model and the actual target values. It quantifies how well or poorly the model is performing. The goal of training a model is to minimize the loss function. The **sparse_categorical_crossentropy loss function** is used in machine learning for multi-class classification problems where the target labels are integers (i.e., not one-hot encoded).\n",
        "\n",
        "#Learning Rate\n",
        "This means how much to adjust the weights based upon the loss.  A larger number will make the model conver quicker, but it that has problems as it could skip over function that have more than one perceived minimum this picking the wrong minimum thus leading to the wrong answer.  The very small learing rate would mean the model takes longer to converge.  \n",
        "\n",
        "#Make a Single Perceptron\n",
        "\n",
        "Here we take Arthur Arnx's example solution to the XOR problem and add explanations to make it easier to understand.  It has three neurons and no hidden layer. So it's 1 or 0 going in and 1 or 0 going out.  \n",
        "\n"
      ],
      "metadata": {
        "id": "KlI5xkSy4UTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a single-layer perceptron to correctly solve the XOR problem, it must be capable of learning non-linearly separable functions, which a single-layer perceptron cannot do. However, for the AND and OR functions, a single-layer perceptron can solve these problems."
      ],
      "metadata": {
        "id": "OlwTNN52tl3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explanation\n",
        "\n",
        "1. AND Gate Training Data:\n",
        "\n",
        "Inputs and outputs are defined for the AND problem.\n",
        "\n",
        "2. Parameters:\n",
        "\n",
        "* Learning rate, bias, and number of epochs are set.\n",
        "* Weights Initialization:\n",
        "\n",
        "Weights are initialized randomly.\n",
        "3. Training Loop:\n",
        "\n",
        "* For each epoch:\n",
        "  * Iterate over all training examples.\n",
        "  * Compute the weighted sum and apply the activation function.\n",
        "  * Calculate the error and update the weights.\n",
        "  * Track the total error for the epoch.\n",
        "* Print the error every 100 epochs to monitor training progress.\n",
        "* Stop training if the total error becomes zero.\n",
        "\n",
        "5. Testing Function:\n",
        "\n",
        "* test_perceptron(input1, input2): Performs a forward pass using the learned weights and applies the Heaviside step function to predict the output.\n",
        "\n",
        "6. Testing the Model:\n",
        "\n",
        "* The code tests the model with various inputs and compares the predicted output with the expected output.\n",
        "* It prints the test input, expected output, and predicted output, and checks if the prediction is correct.\n",
        "\n",
        "##Running the Test\n",
        "By running the complete code, you can train the perceptron and test it with different inputs for the AND gate problem. The output will indicate whether the model correctly predicts the output for each test input. This way, you can verify the functionality of the perceptron with the AND gate."
      ],
      "metadata": {
        "id": "l98i1M61valL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Define the inputs and outputs for the AND problem\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "\n",
        "outputs = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initialize parameters\n",
        "lr = 0.1  # Learning rate\n",
        "bias = 1\n",
        "epochs = 1000\n",
        "\n",
        "# Initialize weights randomly\n",
        "weights = [random.random(), random.random(), random.random()]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "    for i in range(len(inputs)):\n",
        "        input1 = inputs[i][0]\n",
        "        input2 = inputs[i][1]\n",
        "        output = outputs[i]\n",
        "\n",
        "        # Forward pass: calculate weighted sum of inputs and bias\n",
        "        outputP = input1 * weights[0] + input2 * weights[1] + bias * weights[2]\n",
        "\n",
        "        # Activation function: Heaviside step function\n",
        "        if outputP > 0:\n",
        "            outputP = 1\n",
        "        else:\n",
        "            outputP = 0\n",
        "\n",
        "        # Calculate error\n",
        "        error = output - outputP\n",
        "        total_error += abs(error)\n",
        "\n",
        "        # Update weights\n",
        "        weights[0] += error * input1 * lr\n",
        "        weights[1] += error * input2 * lr\n",
        "        weights[2] += error * bias * lr\n",
        "\n",
        "    # Print error at each epoch\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1} \\t Error: {total_error}\")\n",
        "\n",
        "    # Check for convergence\n",
        "    if total_error == 0:\n",
        "        print(\"Converged!\")\n",
        "        break\n",
        "\n",
        "print(\"Final weights:\", weights)\n",
        "\n",
        "# Define a function to test the model with a given input\n",
        "def test_perceptron(input1, input2):\n",
        "    # Forward pass: calculate weighted sum of inputs and bias\n",
        "    outputP = input1 * weights[0] + input2 * weights[1] + bias * weights[2]\n",
        "\n",
        "    # Activation function: Heaviside step function\n",
        "    if outputP > 0:\n",
        "        outputP = 1\n",
        "    else:\n",
        "        outputP = 0\n",
        "\n",
        "    return outputP\n",
        "\n",
        "# Test the model with various inputs\n",
        "test_cases = [\n",
        "    (0, 0, 0),\n",
        "    (0, 1, 0),\n",
        "    (1, 0, 0),\n",
        "    (1, 1, 1)\n",
        "]\n",
        "\n",
        "for test_input1, test_input2, expected_output in test_cases:\n",
        "    predicted_output = test_perceptron(test_input1, test_input2)\n",
        "    print(f\"Test input: [{test_input1}, {test_input2}]\")\n",
        "    print(f\"Expected output: {expected_output}\")\n",
        "    print(f\"Predicted output: {predicted_output}\")\n",
        "    if predicted_output == expected_output:\n",
        "        print(\"The model correctly predicts the output.\\n\")\n",
        "    else:\n",
        "        print(\"The model does not correctly predict the output.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rLcyoYBuRbC",
        "outputId": "fc1ed1e4-7117-4a35-b4a0-446127385ecd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t Error: 3\n",
            "Converged!\n",
            "Final weights: [0.414495426706317, 0.11419809765335864, -0.49757664145338365]\n",
            "Test input: [0, 0]\n",
            "Expected output: 0\n",
            "Predicted output: 0\n",
            "The model correctly predicts the output.\n",
            "\n",
            "Test input: [0, 1]\n",
            "Expected output: 0\n",
            "Predicted output: 0\n",
            "The model correctly predicts the output.\n",
            "\n",
            "Test input: [1, 0]\n",
            "Expected output: 0\n",
            "Predicted output: 0\n",
            "The model correctly predicts the output.\n",
            "\n",
            "Test input: [1, 1]\n",
            "Expected output: 1\n",
            "Predicted output: 1\n",
            "The model correctly predicts the output.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Sigmoid\n",
        "\n",
        "Using the sigmoid function as the activation function requires some adjustments to both the training and testing phases. The sigmoid function outputs values between 0 and 1, so we will need to use a threshold to decide whether the perceptron's output is closer to 0 or"
      ],
      "metadata": {
        "id": "02WghtgCvIy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sigmoid function\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Define the inputs and outputs for the AND problem\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "\n",
        "outputs = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initialize parameters\n",
        "lr = 0.1  # Learning rate\n",
        "bias = 1\n",
        "epochs = 10000\n",
        "\n",
        "# Initialize weights randomly\n",
        "weights = [random.random(), random.random(), random.random()]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "    for i in range(len(inputs)):\n",
        "        input1 = inputs[i][0]\n",
        "        input2 = inputs[i][1]\n",
        "        output = outputs[i]\n",
        "\n",
        "        # Forward pass: calculate weighted sum of inputs and bias\n",
        "        weighted_sum = input1 * weights[0] + input2 * weights[1] + bias * weights[2]\n",
        "\n",
        "        # Apply sigmoid activation function\n",
        "        outputP = sigmoid(weighted_sum)\n",
        "\n",
        "        # Calculate error\n",
        "        error = output - outputP\n",
        "        total_error += abs(error)\n",
        "\n",
        "        # Calculate delta (error * sigmoid_derivative)\n",
        "        delta = error * sigmoid_derivative(outputP)\n",
        "\n",
        "        # Update weights\n",
        "        weights[0] += delta * input1 * lr\n",
        "        weights[1] += delta * input2 * lr\n",
        "        weights[2] += delta * bias * lr\n",
        "\n",
        "    # Print error at each epoch\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch+1} \\t Error: {total_error}\")\n",
        "\n",
        "    # Check for convergence\n",
        "    if total_error < 1e-5:  # Convergence criteria\n",
        "        print(\"Converged!\")\n",
        "        break\n",
        "\n",
        "print(\"Final weights:\", weights)\n",
        "\n",
        "# Define a function to test the model with a given input\n",
        "def test_perceptron(input1, input2):\n",
        "    # Forward pass: calculate weighted sum of inputs and bias\n",
        "    weighted_sum = input1 * weights[0] + input2 * weights[1] + bias * weights[2]\n",
        "\n",
        "    # Apply sigmoid activation function\n",
        "    outputP = sigmoid(weighted_sum)\n",
        "\n",
        "    # Use a threshold to determine the final output (0 or 1)\n",
        "    if outputP > 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Test the model with various inputs\n",
        "test_cases = [\n",
        "    (0, 0, 0),\n",
        "    (0, 1, 0),\n",
        "    (1, 0, 0),\n",
        "    (1, 1, 1)\n",
        "]\n",
        "\n",
        "for test_input1, test_input2, expected_output in test_cases:\n",
        "    predicted_output = test_perceptron(test_input1, test_input2)\n",
        "    print(f\"Test input: [{test_input1}, {test_input2}]\")\n",
        "    print(f\"Expected output: {expected_output}\")\n",
        "    print(f\"Predicted output: {predicted_output}\")\n",
        "    if predicted_output == expected_output:\n",
        "        print(\"The model correctly predicts the output.\\n\")\n",
        "    else:\n",
        "        print(\"The model does not correctly predict the output.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdvfZAJcuzTo",
        "outputId": "54c2f3fa-8ed0-4985-d8e3-f5d81e22890d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t Error: 2.4556852197233354\n",
            "Epoch 1001 \t Error: 0.6294930979876645\n",
            "Epoch 2001 \t Error: 0.43547063092688776\n",
            "Epoch 3001 \t Error: 0.34745748332905735\n",
            "Epoch 4001 \t Error: 0.29564468027080426\n",
            "Epoch 5001 \t Error: 0.2608602695241691\n",
            "Epoch 6001 \t Error: 0.23557525167747478\n",
            "Epoch 7001 \t Error: 0.21618945841715168\n",
            "Epoch 8001 \t Error: 0.20074820148190664\n",
            "Epoch 9001 \t Error: 0.1880910777666297\n",
            "Final weights: [5.482220333554342, 5.481856657916964, -8.31506764646324]\n",
            "Test input: [0, 0]\n",
            "Expected output: 0\n",
            "Predicted output: 0\n",
            "The model correctly predicts the output.\n",
            "\n",
            "Test input: [0, 1]\n",
            "Expected output: 0\n",
            "Predicted output: 0\n",
            "The model correctly predicts the output.\n",
            "\n",
            "Test input: [1, 0]\n",
            "Expected output: 0\n",
            "Predicted output: 0\n",
            "The model correctly predicts the output.\n",
            "\n",
            "Test input: [1, 1]\n",
            "Expected output: 1\n",
            "Predicted output: 1\n",
            "The model correctly predicts the output.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}