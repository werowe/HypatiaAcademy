{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf/5fzyRy3+mgYJHqBdl5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/tensorflow_tape_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conver this code to TensorFlow so we can use GradientTape\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "\n",
        "  return  1 / (1 + np.exp(-z))\n",
        "\n",
        "def loss(ŷ,y):\n",
        "\n",
        "  return - (y * np.log(ŷ) + (1 - y) * np.log(1 - ŷ))\n",
        "\n",
        "\n",
        "# input features\n",
        "x = np.array([1,2,3])\n",
        "\n",
        "# actual observed output value\n",
        "y = np.array(1)\n",
        "\n",
        "# initialize weights\n",
        "w = np.random.rand(3)\n",
        "\n",
        "print(\"\\ninitial weights\", w)\n",
        "\n",
        "# initialize bias b\n",
        "b = 0\n",
        "\n",
        "print(\"\\ninitial bias\", b)\n",
        "\n",
        "\n",
        "def forward_pass(w,x,b,y):\n",
        "\n",
        "  # calculate z, the linear output\n",
        "  z = np.dot(w,x) + b\n",
        "\n",
        "  #print(\"\\nz\", z)\n",
        "\n",
        "  # calculate the activation,  this is also the prediction ŷ\n",
        "  ŷ = sigmoid(z)\n",
        "  #print(\"\\nprediction ŷ\", ŷ)\n",
        "\n",
        "  # calculate the loss\n",
        "  l = loss(ŷ,y)\n",
        "\n",
        "  #print(\"\\nloss l\", l)\n",
        "\n",
        "  return ŷ, y, w, b, l\n",
        "\n",
        "# gradients\n",
        "\n",
        "def back_progagation(ŷ, y, w, b):\n",
        "\n",
        "  db =  ŷ - y\n",
        "\n",
        "  dw = (ŷ - y) * x\n",
        "\n",
        "  # calculate gradients\n",
        "  #print(\"\\ndb\", db)\n",
        "\n",
        "  #print(\"dw\", dw)\n",
        "\n",
        "  # learning rate\n",
        "  lr = 0.1\n",
        "\n",
        "  # update weight and bias\n",
        "  b = b - lr * db\n",
        "  #print(\"\\nnew bias b\", b)\n",
        "\n",
        "  w = w - lr * dw\n",
        "  #print(\"\\nnew weights w\", w)\n",
        "\n",
        "  return w , b\n",
        "\n",
        "# how many times to loop\n",
        "cnt = 100\n",
        "\n",
        "while cnt > 0:\n",
        "  ŷ, y, w, b, l = forward_pass(w,x,b,y)\n",
        "  if cnt%5 ==0:\n",
        "    print(\"loss\", l)\n",
        "  w , b= back_progagation(ŷ, y, w, b)\n",
        "  cnt -= 1\n",
        "\n",
        "print(\"\\nfinal weights\", w)\n",
        "print(\"\\nfinal bias\", b)\n",
        "\n",
        "print(\"\\nnow make prediction\")\n",
        "\n",
        "ŷ = sigmoid(np.dot(w,x)+b)\n",
        "print(\"\\npredicted value ŷ\", ŷ)\n",
        "\n",
        "print(\"\\nobserved value y\", y)\n",
        "\n",
        "\n",
        "# and since it's logistic regression\n",
        "\n",
        "if ŷ > 0.5:\n",
        "  print(\"\\nlogistic regression\", True)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "RKCO74c2KYFj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tns11b8e_Str",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc41d88-2480-4b86-eac6-6f3fa37b220b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "initial values\n",
            "weights w <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0.1925441 , 0.7286116 , 0.85141337], dtype=float32)>\n",
            "bias b <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
            "loss tf.Tensor(0.0148251625, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.012135675, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.010271241, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.008902902, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.007856005, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.007029232, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.006359796, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.005806705, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.0053420626, shape=(), dtype=float32)\n",
            "loss tf.Tensor(0.004946233, shape=(), dtype=float32)\n",
            "\n",
            "final weights w <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0.27083123, 0.8851857 , 1.0862744 ], dtype=float32)>\n",
            "\n",
            "final bias <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.07828710973262787>\n",
            "test prediction\n",
            "\n",
            "prediction tf.Tensor(0.9954056, shape=(), dtype=float32)  actual value tf.Tensor(1.0, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "x = tf.constant([1, 2, 3], dtype=tf.float32)     # Features\n",
        "y = tf.constant(1, dtype=tf.float32)             # Target\n",
        "w = tf.Variable(tf.random.uniform(shape=[3], minval=0.0, maxval=1.0, dtype=tf.float32))\n",
        "b = tf.Variable(0.0, dtype=tf.float32)           # Bias\n",
        "\n",
        "print(\"\\ninitial values\")\n",
        "print(\"weights w\", w)\n",
        "print(\"bias b\", b)\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        z = tf.tensordot(w, x, axes=1) + b\n",
        "        y_hat = tf.math.sigmoid(z)\n",
        "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=z)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "    dw = tape.gradient(loss, w)\n",
        "    db = tape.gradient(loss, b)\n",
        "    w.assign_sub(lr * dw)\n",
        "    b.assign_sub(lr * db)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print(\"loss\", loss)\n",
        "\n",
        "\n",
        "print(\"\\nfinal weights w\", w)\n",
        "print(\"\\nfinal bias\", b)\n",
        "\n",
        "print(\"test prediction\")\n",
        "\n",
        "z = tf.tensordot(w, x, axes=1) + b\n",
        "y_hat = tf.math.sigmoid(z)\n",
        "\n",
        "print(\"\\nprediction\", y_hat, \" actual value\", y)"
      ]
    }
  ]
}