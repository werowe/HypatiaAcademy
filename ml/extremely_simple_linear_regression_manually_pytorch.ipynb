{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdmUWNNR7vwYF/a9LE4xad",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/extremely_simple_linear_regression_manually_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the training step. Here’s how it works:\n",
        "\n",
        "1. loss.backward() runs backpropagation starting from the loss output node, and populates the tensor.grad attribute on all tensors that were involved in the computation of loss. tensor.grad represents the gradient of the loss with regard to that tensor.\n",
        "\n",
        "2. We use the .grad attribute to recover the gradients of the loss with regard to W and b.\n",
        "\n",
        "3. We update W and b using those gradients. Because these updates are not intended to be part of the backwards pass, we do them inside a torch.no_grad() scope, which skips gradient computation for everything inside it.\n",
        "\n",
        "4. We reset the contents of the .grad property of our W and b parameters, by setting it None. If we didn’t do this, gradient values would accumulate across multiple calls to training_step(), resulting in invalid values."
      ],
      "metadata": {
        "id": "J3ujr14n9Bdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "epochs = 8\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Data\n",
        "X = torch.tensor([[1.0]])\n",
        "y = 2 * X  + 1\n",
        "\n",
        "# Parameters\n",
        "W = torch.tensor([[2.3]], requires_grad=True)\n",
        "b = torch.tensor(1.4, requires_grad=True)\n"
      ],
      "metadata": {
        "id": "o6vBqnF3mwNF"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_training(X, y, W, b):\n",
        "  # Forward pass\n",
        "  predictions = torch.matmul(X, W) + b\n",
        "  loss = torch.square(y - predictions).sum()  # Ensure scalar loss\n",
        "  print(\"\\nloss\", loss)\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weights\n",
        "  with torch.no_grad():\n",
        "    W -= W.grad * learning_rate\n",
        "    b -= b.grad * learning_rate\n",
        "\n",
        "  print(\"W gradient:\", W.grad.item())\n",
        "  print(\"b gradient:\", b.grad.item())\n",
        "  print(\"W after:\", W.item())\n",
        "  print(\"b after:\", b.item())\n",
        "\n",
        "  # Zero gradients (optional, for next iteration)\n",
        "  W.grad = None\n",
        "  b.grad = None\n",
        "\n"
      ],
      "metadata": {
        "id": "Bp965kjh-DTn"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  run_training(X, y, W, b)"
      ],
      "metadata": {
        "id": "RHEQ71J4k4Cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80838fdb-0c6b-4b22-d23b-2cb89d65812d"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loss tensor(0.4900, grad_fn=<SumBackward0>)\n",
            "W gradient: 1.3999996185302734\n",
            "b gradient: 1.3999996185302734\n",
            "W after: 2.1600000858306885\n",
            "b after: 1.2599999904632568\n",
            "\n",
            "loss tensor(0.1764, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.8400001525878906\n",
            "b gradient: 0.8400001525878906\n",
            "W after: 2.0759999752044678\n",
            "b after: 1.1759999990463257\n",
            "\n",
            "loss tensor(0.0635, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.5039997100830078\n",
            "b gradient: 0.5039997100830078\n",
            "W after: 2.025599956512451\n",
            "b after: 1.125599980354309\n",
            "\n",
            "loss tensor(0.0229, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.3023996353149414\n",
            "b gradient: 0.3023996353149414\n",
            "W after: 1.995360016822815\n",
            "b after: 1.0953600406646729\n",
            "\n",
            "loss tensor(0.0082, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.1814403533935547\n",
            "b gradient: 0.1814403533935547\n",
            "W after: 1.9772160053253174\n",
            "b after: 1.0772160291671753\n",
            "\n",
            "loss tensor(0.0030, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.10886383056640625\n",
            "b gradient: 0.10886383056640625\n",
            "W after: 1.966329574584961\n",
            "b after: 1.0663295984268188\n",
            "\n",
            "loss tensor(0.0011, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.06531810760498047\n",
            "b gradient: 0.06531810760498047\n",
            "W after: 1.959797739982605\n",
            "b after: 1.059797763824463\n",
            "\n",
            "loss tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.039191246032714844\n",
            "b gradient: 0.039191246032714844\n",
            "W after: 1.9558786153793335\n",
            "b after: 1.0558786392211914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8yDA-ezssvU",
        "outputId": "3cd75a56-21a8-4727-8e66-25108cfa83d4"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    }
  ]
}