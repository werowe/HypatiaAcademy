{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQtzboQmZusTKPJ6x1hXbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/extremely_simple_linear_regression_manually_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the training step. Here’s how it works:\n",
        "\n",
        "1. loss.backward() runs backpropagation starting from the loss output node, and populates the tensor.grad attribute on all tensors that were involved in the computation of loss. tensor.grad represents the gradient of the loss with regard to that tensor.\n",
        "\n",
        "2. We use the .grad attribute to recover the gradients of the loss with regard to W and b.\n",
        "\n",
        "3. We update W and b using those gradients. Because these updates are not intended to be part of the backwards pass, we do them inside a torch.no_grad() scope, which skips gradient computation for everything inside it.\n",
        "\n",
        "4. We reset the contents of the .grad property of our W and b parameters, by setting it None. If we didn’t do this, gradient values would accumulate across multiple calls to training_step(), resulting in invalid values."
      ],
      "metadata": {
        "id": "J3ujr14n9Bdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Theoretical Derivative\n",
        "\n",
        "**you're optimizing parameters $ W $ and $ b $ in the model**:\n",
        "\n",
        "$$\n",
        "\\text{prediction} = W \\cdot x + b\n",
        "$$\n",
        "\n",
        "with a loss function (mean squared error):\n",
        "\n",
        "$$\n",
        "\\text{loss} = (y - \\text{prediction})^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. What is the Gradient Actually Calculated?\n",
        "\n",
        "You are calculating the gradient of the loss **with respect to the parameters** $ W $ and $ b $, **not with respect to $ x $**.\n",
        "\n",
        "Let’s compute the gradient with respect to $ W $:\n",
        "\n",
        "Given:\n",
        "- $ x = 1 $\n",
        "- $ y = 2 $\n",
        "- $ \\text{prediction} = W \\cdot x + b $\n",
        "- $ \\text{loss} = (y - \\text{prediction})^2 $\n",
        "\n",
        "The derivative of the loss with respect to $ W $ is:\n",
        "\n",
        "$$\n",
        "\\frac{d(\\text{loss})}{dW} = 2 \\cdot (y - \\text{prediction}) \\cdot (-x)\n",
        "$$\n",
        "\n",
        "So, the gradient depends on the current values of $ W $ and $ b $.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Example Calculation\n",
        "\n",
        "Suppose:\n",
        "- $ W = 0.5 $\n",
        "- $ b = 0 $\n",
        "- $ x = 1 $\n",
        "- $ y = 2 $\n",
        "\n",
        "Then:\n",
        "- $ \\text{prediction} = 0.5 \\times 1 + 0 = 0.5 $\n",
        "- $ \\text{loss} = (2 - 0.5)^2 = 2.25 $\n",
        "- Gradient w.r.t. $ W $:\n",
        "\n",
        "$$\n",
        "\\frac{d(\\text{loss})}{dW} = 2 \\cdot (2 - 0.5) \\cdot (-1) = 2 \\cdot 1.5 \\cdot (-1) = -3\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Up9HhgX6vL78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Data\n",
        "X = torch.tensor([[1.0]])\n",
        "y = 2 * X  + 1\n",
        "\n",
        "# Parameters\n",
        "W = torch.tensor([[2.3]], requires_grad=True)\n",
        "b = torch.tensor(1.4, requires_grad=True)\n"
      ],
      "metadata": {
        "id": "o6vBqnF3mwNF"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_training(X, y, W, b):\n",
        "  # Forward pass\n",
        "  predictions = torch.matmul(X, W) + b\n",
        "  loss = torch.square(y - predictions).sum()  # Ensure scalar loss\n",
        "  print(\"\\nloss\", loss)\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weights\n",
        "  with torch.no_grad():\n",
        "    W -= W.grad * learning_rate\n",
        "    b -= b.grad * learning_rate\n",
        "\n",
        "  print(\"W gradient:\", W.grad.item())\n",
        "  print(\"b gradient:\", b.grad.item())\n",
        "  print(\"W after:\", W.item())\n",
        "  print(\"b after:\", b.item())\n",
        "\n",
        "  # Zero gradients (optional, for next iteration)\n",
        "  W.grad = None\n",
        "  b.grad = None\n",
        "\n"
      ],
      "metadata": {
        "id": "Bp965kjh-DTn"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  run_training(X, y, W, b)"
      ],
      "metadata": {
        "id": "RHEQ71J4k4Cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0effd785-b3d7-4bad-b41f-27c0128b7355"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loss tensor(0.4900, grad_fn=<SumBackward0>)\n",
            "W gradient: 1.3999996185302734\n",
            "b gradient: 1.3999996185302734\n",
            "W after: 2.1600000858306885\n",
            "b after: 1.2599999904632568\n",
            "\n",
            "loss tensor(0.1764, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.8400001525878906\n",
            "b gradient: 0.8400001525878906\n",
            "W after: 2.0759999752044678\n",
            "b after: 1.1759999990463257\n",
            "\n",
            "loss tensor(0.0635, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.5039997100830078\n",
            "b gradient: 0.5039997100830078\n",
            "W after: 2.025599956512451\n",
            "b after: 1.125599980354309\n",
            "\n",
            "loss tensor(0.0229, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.3023996353149414\n",
            "b gradient: 0.3023996353149414\n",
            "W after: 1.995360016822815\n",
            "b after: 1.0953600406646729\n",
            "\n",
            "loss tensor(0.0082, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.1814403533935547\n",
            "b gradient: 0.1814403533935547\n",
            "W after: 1.9772160053253174\n",
            "b after: 1.0772160291671753\n",
            "\n",
            "loss tensor(0.0030, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.10886383056640625\n",
            "b gradient: 0.10886383056640625\n",
            "W after: 1.966329574584961\n",
            "b after: 1.0663295984268188\n",
            "\n",
            "loss tensor(0.0011, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.06531810760498047\n",
            "b gradient: 0.06531810760498047\n",
            "W after: 1.959797739982605\n",
            "b after: 1.059797763824463\n",
            "\n",
            "loss tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.039191246032714844\n",
            "b gradient: 0.039191246032714844\n",
            "W after: 1.9558786153793335\n",
            "b after: 1.0558786392211914\n",
            "\n",
            "loss tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.023514747619628906\n",
            "b gradient: 0.023514747619628906\n",
            "W after: 1.9535270929336548\n",
            "b after: 1.0535271167755127\n",
            "\n",
            "loss tensor(4.9764e-05, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.014108657836914062\n",
            "b gradient: 0.014108657836914062\n",
            "W after: 1.9521162509918213\n",
            "b after: 1.0521162748336792\n",
            "\n",
            "loss tensor(1.7913e-05, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.008464813232421875\n",
            "b gradient: 0.008464813232421875\n",
            "W after: 1.9512697458267212\n",
            "b after: 1.051269769668579\n",
            "\n",
            "loss tensor(6.4497e-06, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.0050792694091796875\n",
            "b gradient: 0.0050792694091796875\n",
            "W after: 1.9507617950439453\n",
            "b after: 1.0507618188858032\n",
            "\n",
            "loss tensor(2.3210e-06, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.0030469894409179688\n",
            "b gradient: 0.0030469894409179688\n",
            "W after: 1.9504570960998535\n",
            "b after: 1.0504571199417114\n",
            "\n",
            "loss tensor(8.3557e-07, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.0018281936645507812\n",
            "b gradient: 0.0018281936645507812\n",
            "W after: 1.9502742290496826\n",
            "b after: 1.0502742528915405\n",
            "\n",
            "loss tensor(3.0070e-07, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.0010967254638671875\n",
            "b gradient: 0.0010967254638671875\n",
            "W after: 1.950164556503296\n",
            "b after: 1.0501645803451538\n",
            "\n",
            "loss tensor(1.0825e-07, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.0006580352783203125\n",
            "b gradient: 0.0006580352783203125\n",
            "W after: 1.9500987529754639\n",
            "b after: 1.0500987768173218\n",
            "\n",
            "loss tensor(3.8971e-08, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.0003948211669921875\n",
            "b gradient: 0.0003948211669921875\n",
            "W after: 1.9500592947006226\n",
            "b after: 1.0500593185424805\n",
            "\n",
            "loss tensor(1.4097e-08, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.00023746490478515625\n",
            "b gradient: 0.00023746490478515625\n",
            "W after: 1.950035572052002\n",
            "b after: 1.0500355958938599\n",
            "\n",
            "loss tensor(5.0479e-09, grad_fn=<SumBackward0>)\n",
            "W gradient: 0.00014209747314453125\n",
            "b gradient: 0.00014209747314453125\n",
            "W after: 1.9500213861465454\n",
            "b after: 1.0500214099884033\n",
            "\n",
            "loss tensor(1.8417e-09, grad_fn=<SumBackward0>)\n",
            "W gradient: 8.58306884765625e-05\n",
            "b gradient: 8.58306884765625e-05\n",
            "W after: 1.9500128030776978\n",
            "b after: 1.0500128269195557\n"
          ]
        }
      ]
    }
  ]
}