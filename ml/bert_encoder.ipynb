{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNti1affSiKx+/0sAFsL4md",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/bert_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding BERT Encoding\n",
        "\n",
        "**BERT (Bidirectional Encoder Representations from Transformers)** is a transformer-based model designed to pre-train deep bidirectional representations from the unlabeled text by jointly conditioning on both left and right context in all layers. Hereâ€™s a breakdown of how BERT encoding works:\n",
        "\n",
        "### How BERT Reads Text\n",
        "\n",
        "1. **Bidirectional Context**: Unlike traditional models like LSTMs that read text sequentially either left-to-right or right-to-left, BERT reads text in both directions. It captures the context from both directions simultaneously using transformers.\n",
        "\n",
        "2. **Transformers**: BERT is based on the Transformer architecture, which uses self-attention mechanisms to weigh the importance of different words in a sentence regardless of their position. This allows it to build a rich contextual representation of words.\n",
        "\n",
        "### Encoding Process\n",
        "\n",
        "1. **Tokenization**:\n",
        "   - The text is split into tokens. BERT uses WordPiece tokenization, where words are split into subwords or characters when necessary.\n",
        "   - Special tokens are added: `[CLS]` at the beginning of a sequence and `[SEP]` at the end of a sequence or to separate different sequences.\n",
        "\n",
        "   Example:\n",
        "   ```\n",
        "   Input: \"I love programming.\"\n",
        "   Tokenized: ['[CLS]', 'i', 'love', 'programming', '.', '[SEP]']\n",
        "   ```\n",
        "\n",
        "2. **Embedding**:\n",
        "   - **Token Embeddings**: Each token is converted into a dense vector.\n",
        "   - **Segment Embeddings**: For tasks involving pairs of sentences, segment embeddings differentiate the two sentences.\n",
        "   - **Positional Embeddings**: Since transformers don't have inherent order information, positional embeddings are added to each token to encode its position in the sequence.\n",
        "\n",
        "   Example:\n",
        "   ```\n",
        "   Token Embedding: [v_i, v_love, v_programming, v_.]\n",
        "   Segment Embedding: [v_segment_A, v_segment_A, v_segment_A, v_segment_A]\n",
        "   Positional Embedding: [v_pos_0, v_pos_1, v_pos_2, v_pos_3]\n",
        "   Combined Embedding: v_combined_0, v_combined_1, v_combined_2, v_combined_3\n",
        "   ```\n",
        "\n",
        "3. **Self-Attention Mechanism**:\n",
        "   - Each token attends to every other token in the sequence to build a contextual understanding.\n",
        "   - This is done using multiple attention heads, allowing the model to focus on different parts of the sentence simultaneously.\n",
        "\n",
        "4. **Transformer Layers**:\n",
        "   - The combined embeddings are passed through multiple transformer layers. Each layer consists of a multi-head self-attention mechanism followed by position-wise feed-forward neural networks.\n",
        "   - The output of each layer is passed to the next, allowing the model to build increasingly complex representations.\n",
        "\n",
        "### Connecting Sentences\n",
        "\n",
        "- BERT can handle pairs of sentences by including a `[SEP]` token to differentiate between them. During training for tasks like Question Answering or Next Sentence Prediction, it learns to understand the relationship between these sentences.\n",
        "\n",
        "   Example:\n",
        "   ```\n",
        "   Sentence A: \"How are you?\"\n",
        "   Sentence B: \"I am fine.\"\n",
        "   Tokenized: ['[CLS]', 'how', 'are', 'you', '?', '[SEP]', 'i', 'am', 'fine', '.', '[SEP]']\n",
        "   ```\n",
        "\n",
        "### Integration into Neural Networks\n",
        "\n",
        "**Using BERT in Downstream Tasks**:\n",
        "\n",
        "1. **Feature Extraction**:\n",
        "   - BERT can be used as a fixed feature extractor where the encoded representations of the input text are taken from a specific layer and used in downstream tasks.\n",
        "   - The `[CLS]` token's representation is often used as a summary of the entire sequence for classification tasks.\n",
        "\n",
        "   Example:\n",
        "   ```\n",
        "   Encoded Representation: [v_CLS, v_how, v_are, v_you, v_?, v_SEP, v_i, v_am, v_fine, v_.]\n",
        "   ```\n",
        "\n",
        "2. **Fine-Tuning**:\n",
        "   - BERT can be fine-tuned for specific tasks by adding task-specific layers on top of the pre-trained model and training the entire architecture.\n",
        "   - For classification, a dense layer followed by a softmax layer can be added on top of the `[CLS]` token's output.\n",
        "\n",
        "   Example:\n",
        "   ```\n",
        "   Model Architecture:\n",
        "   Input: [CLS] How are you? [SEP] I am fine. [SEP]\n",
        "   BERT Encoder Layers\n",
        "   Output (of [CLS] token): v_CLS\n",
        "   Dense Layer: Dense(v_CLS)\n",
        "   Softmax Layer: Softmax(Dense(v_CLS))\n",
        "   ```\n",
        "\n",
        "### Examples\n",
        "\n",
        "1. **Single Sentence Encoding**:\n",
        "   - Input: \"The quick brown fox.\"\n",
        "   - Tokenized: ['[CLS]', 'the', 'quick', 'brown', 'fox', '.', '[SEP]']\n",
        "   - Embedding + Self-Attention -> Encoded Representation\n",
        "\n",
        "2. **Sentence Pair Encoding**:\n",
        "   - Input: \"What is your name?\" \"My name is BERT.\"\n",
        "   - Tokenized: ['[CLS]', 'what', 'is', 'your', 'name', '?', '[SEP]', 'my', 'name', 'is', 'bert', '.', '[SEP]']\n",
        "   - Embedding + Self-Attention -> Encoded Representation\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "BERT's ability to read text bidirectionally and its use of transformers for self-attention allows it to create rich, contextual representations of text. These encoded representations can be used as features in downstream tasks or fine-tuned with additional layers to improve performance on specific tasks."
      ],
      "metadata": {
        "id": "sFo_JEGuCpi9"
      }
    }
  ]
}