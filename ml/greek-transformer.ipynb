{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxMnHFa8K/Xopn7aBxOHEn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/greek-transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrCNQzlrr8tZ",
        "outputId": "cc2bad89-50ad-45b1-d7bf-612178c2e738"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.cs.aueb.gr/software_and_datasets/greek_bert_ner_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rigp1F8dxAH1",
        "outputId": "31e254dc-756e-4a8e-d852-474ebf3b0be0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-26 08:58:00--  http://nlp.cs.aueb.gr/software_and_datasets/greek_bert_ner_data.zip\n",
            "Resolving nlp.cs.aueb.gr (nlp.cs.aueb.gr)... 195.251.248.252\n",
            "Connecting to nlp.cs.aueb.gr (nlp.cs.aueb.gr)|195.251.248.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 389433 (380K) [application/zip]\n",
            "Saving to: ‘greek_bert_ner_data.zip’\n",
            "\n",
            "greek_bert_ner_data 100%[===================>] 380.31K   163KB/s    in 2.3s    \n",
            "\n",
            "2024-06-26 08:58:04 (163 KB/s) - ‘greek_bert_ner_data.zip’ saved [389433/389433]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.analyticsvidhya.com/blog/2021/09/a-deep-dive-into-transformers-library/"
      ],
      "metadata": {
        "id": "ufQe3LmS0FB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2021/09/a-deep-dive-into-transformers-library/"
      ],
      "metadata": {
        "id": "kmtK6I3D0GxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://sparknlp.org/2021/09/07/bert_base_uncased_el.html"
      ],
      "metadata": {
        "id": "V7o3lBfAyEzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import unicodedata\n",
        "\n",
        "def strip_accents_and_lowercase(s):\n",
        "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                  if unicodedata.category(c) != 'Mn').lower()\n",
        "\n",
        "accented_string = \"Αυτή είναι η Ελληνική έκδοση του BERT.\"\n",
        "unaccented_string = strip_accents_and_lowercase(accented_string)\n",
        "\n",
        "print(unaccented_string) # αυτη ειναι η ελληνικη εκδοση του bert.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yBi5-ZbwphL",
        "outputId": "af5f4009-4e99-415e-b364-ad82be5457a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "αυτη ειναι η ελληνικη εκδοση του bert.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2vLPGyMrDN0",
        "outputId": "045e846a-86db-44f1-d110-159583f3fd10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/pytorch_model.bin\n",
            "Some weights of the model checkpoint at nlpaueb/bert-base-greek-uncased-v1 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at nlpaueb/bert-base-greek-uncased-v1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\")\n",
        "model = AutoModel.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================ EXAMPLE 1 ================\n",
        "text_1 = 'O ποιητής έγραψε ένα [MASK] .'\n",
        "# EN: 'The poet wrote a [MASK].'\n",
        "input_ids = tokenizer_greek.encode(text_1)\n",
        "print(tokenizer_greek.convert_ids_to_tokens(input_ids))\n",
        "# ['[CLS]', 'o', 'ποιητης', 'εγραψε', 'ενα', '[MASK]', '.', '[SEP]']\n",
        "outputs = lm_model_greek(torch.tensor([input_ids]))[0]\n",
        "print(tokenizer_greek.convert_ids_to_tokens(outputs[0, 5].max(0)[1].item()))\n",
        "# the most plausible prediction for [MASK] is \"song\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRNMRszSulK0",
        "outputId": "d2394eb4-e82a-4dae-f070-8b3d6a67080a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'o', 'ποιητης', 'εγραψε', 'ενα', '[MASK]', '.', '[SEP]']\n",
            "τραγουδι\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export HF_TOKEN=\"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\""
      ],
      "metadata": {
        "id": "6WncFT0Jrg-v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "# Define the model checkpoint\n",
        "checkpoint = \"nlpaueb/bert-base-greek-uncased-v1\"\n",
        "# Download and cache the tokenizer and classification model\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "# Define the inputs and tokenize them\n",
        "raw_inputs = [\n",
        "    \"Είμαι πολύ θυμωμένος γιατί έχασα τα κλειδιά μου\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "# Get the outputs from the model\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n",
        "# Find the class/label probabilities\n",
        "outputs = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
        "print(outputs)\n",
        "# Find the label to class mapping for verification\n",
        "print(model.config.id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rJImOhqvYFK",
        "outputId": "3ea215eb-cec4-4577-95ad-51b5eab8f024"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"nlpaueb/bert-base-greek-uncased-v1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--nlpaueb--bert-base-greek-uncased-v1/snapshots/ec2b8f88dd215b5246f2f850413d5bff90d7540d/pytorch_model.bin\n",
            "Some weights of the model checkpoint at nlpaueb/bert-base-greek-uncased-v1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/bert-base-greek-uncased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,   704,   388, 29532,   278,   425, 21240,   355,  6925,   386,\n",
            "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6341,  0.0874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "tensor([[0.3271, 0.6729]], grad_fn=<SoftmaxBackward0>)\n",
            "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
          ]
        }
      ]
    }
  ]
}