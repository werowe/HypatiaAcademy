{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpPLfxjcW1Zt0C5+39YAsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/simple_linear_regression_keras_neural_networkd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Linear Regression with Keras\n",
        "\n",
        "\n",
        "##Activation Function in Linear Regression for Nueral Networks\n",
        "\n",
        "If no **activation function** is specified in a Keras `Dense` layer the layer uses the default: a linear activation, also called the identity function. This means the output of the neuron is simply the weighted sum of its inputs plus a bias, with no non-linear transformation applied.\n",
        "Implications:\n",
        "\n",
        "• The model is purely linear. With only linear activations (or no activation), the network is mathematically equivalent to a linear regression model, regardless of how many layers you stack.\n",
        "\n",
        "• The model cannot learn or represent any non-linear relationships in your data. All it can do is fit a straight line (or a hyperplane in higher dimensions) to your data.\n",
        "\n",
        "• If you add more layers, they collapse into a single linear transformation, so the “depth” of the network is meaningless without non-linear activations."
      ],
      "metadata": {
        "id": "83ryCThnPW3S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLf8Wh3dw46J",
        "outputId": "a0a85a4c-bbca-40ec-bd34-334545b033a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\n",
            "Original vs Predicted:\n",
            "[[ 0.          0.          0.03745208]\n",
            " [ 1.          3.          3.0325346 ]\n",
            " [ 2.          6.          6.02761745]\n",
            " [ 3.          9.          9.02269936]\n",
            " [ 4.         12.         12.01778221]\n",
            " [ 5.         15.         15.01286507]\n",
            " [ 6.         18.         18.00794792]\n",
            " [ 7.         21.         21.00303078]\n",
            " [ 8.         24.         23.99811363]\n",
            " [ 9.         27.         26.99319649]\n",
            " [10.         30.         29.98827934]\n",
            " [11.         33.         32.98336029]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Data generation\n",
        "x = np.arange(12).reshape(-1, 1)\n",
        "y = 3 * x\n",
        "\n",
        "# don't use scaler with small amount of data and linear model\n",
        "# it's overkill and not likely to work\n",
        "\n",
        "# Model setup\n",
        "#  # Single neuron with input dimension of 1\n",
        "model = Sequential()\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer=SGD(learning_rate=0.01), loss='mse')\n",
        "\n",
        "# Training\n",
        "history = model.fit(x, y, epochs=500, verbose=0)\n",
        "\n",
        "# Predictions (with inverse scaling)\n",
        "predictions = model.predict(x)\n",
        "\n",
        "\n",
        "# Results\n",
        "print(\"\\nOriginal vs Predicted:\")\n",
        "print(np.column_stack([x, y, predictions]))\n"
      ]
    }
  ]
}