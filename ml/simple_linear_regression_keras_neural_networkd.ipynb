{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjMKJD8BElJ16BNsTbBmzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/simple_linear_regression_keras_neural_networkd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Linear Regression with Keras\n",
        "\n",
        "\n",
        "##Activation Function in Linear Regression for Nueral Networks\n",
        "\n",
        "If no **activation function** is specified in a Keras `Dense` layer the layer uses the default: a linear activation, also called the identity function. This means the output of the neuron is simply the weighted sum of its inputs plus a bias, with no non-linear transformation applied.\n",
        "Implications:\n",
        "\n",
        "• The model is purely linear. With only linear activations (or no activation), the network is mathematically equivalent to a linear regression model, regardless of how many layers you stack.\n",
        "\n",
        "• The model cannot learn or represent any non-linear relationships in your data. All it can do is fit a straight line (or a hyperplane in higher dimensions) to your data.\n",
        "\n",
        "• If you add more layers, they collapse into a single linear transformation, so the “depth” of the network is meaningless without non-linear activations."
      ],
      "metadata": {
        "id": "83ryCThnPW3S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLf8Wh3dw46J",
        "outputId": "39adb073-982a-413a-e671-6af059b04f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\n",
            "Original vs Predicted:\n",
            "[[ 0.          0.          4.07099056]\n",
            " [ 1.          3.          6.38816357]\n",
            " [ 2.          6.          8.70533752]\n",
            " [ 3.          9.         11.02251148]\n",
            " [ 4.         12.         13.33968449]\n",
            " [ 5.         15.         15.65685749]\n",
            " [ 6.         18.         17.97403145]\n",
            " [ 7.         21.         20.29120445]\n",
            " [ 8.         24.         22.60837746]\n",
            " [ 9.         27.         24.92555046]\n",
            " [10.         30.         27.24272346]\n",
            " [11.         33.         29.55989647]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "\n",
        "# Data generation\n",
        "x = np.arange(12).reshape(-1, 1)\n",
        "y = 3 * x\n",
        "\n",
        "# Normalization\n",
        "scaler_x = MinMaxScaler()\n",
        "x_scaled = scaler_x.fit_transform(x)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Model setup\n",
        "#  # Single neuron with input dimension of 1\n",
        "model = Sequential()\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer=SGD(learning_rate=0.01), loss='mse')\n",
        "\n",
        "# Training\n",
        "history = model.fit(x_scaled, y_scaled, epochs=1000, verbose=0)\n",
        "\n",
        "# Predictions (with inverse scaling)\n",
        "predictions_scaled = model.predict(x_scaled)\n",
        "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
        "\n",
        "# Results\n",
        "print(\"\\nOriginal vs Predicted:\")\n",
        "print(np.column_stack([x, y, predictions]))\n"
      ]
    }
  ]
}