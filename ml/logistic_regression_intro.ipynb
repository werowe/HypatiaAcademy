{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSFXHYccQC0rV6lPraWRXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/logistic_regression_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Log Odds\n",
        "\n",
        "The **log odds** is the natural logarithm of the odds, which provides a way to express the relationship between probabilities in a continuous scale. Letâ€™s break this down using the example of the horse with odds of 9 to 1:\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Recall the Odds**\n",
        "Odds are expressed as the ratio of the probability of an event happening to the probability of it not happening. For odds of 9 to 1:\n",
        "- Probability of winning: $$ P(\\text{Win}) = \\frac{1}{10} = 0.1 $$\n",
        "- Probability of losing: $$ P(\\text{Lose}) = 1 - P(\\text{Win}) = 0.9 $$\n",
        "\n",
        "The odds are calculated as:\n",
        "$$\n",
        "\\text{Odds} = \\frac{P(\\text{Win})}{P(\\text{Lose})} = \\frac{0.1}{0.9} = \\frac{1}{9}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Log Odds**\n",
        "The **log odds** is simply the natural logarithm ($$\\ln$$) of the odds:\n",
        "$$\n",
        "\\text{Log Odds} = \\ln(\\text{Odds}) = \\ln\\left(\\frac{1}{9}\\right).\n",
        "$$\n",
        "\n",
        "Using a calculator:\n",
        "$$\n",
        "\\ln\\left(\\frac{1}{9}\\right) = \\ln(1) - \\ln(9) = 0 - 2.197 = -2.197.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation**\n",
        "- A **negative log odds** ($$-2.197$$) indicates that the event (the horse winning) is less likely than its complement (the horse losing). The further negative the log odds, the less likely the event is.\n",
        "- If log odds were **positive**, it would mean the event is more likely than its complement.\n",
        "\n",
        "---\n",
        "\n",
        "### **In Summary**\n",
        "- Odds quantify how likely an event is relative to its complement.\n",
        "- Log odds provide a continuous scale for measuring likelihood, which is useful in statistical modeling (e.g., logistic regression).\n"
      ],
      "metadata": {
        "id": "2vDxpuSErFsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the linear function in logistic regression is expressed as $$ mx + b $$ (where $m$ is the slope and $b$ is the intercept), we can derive the **logistic regression function** step by step. Here's the explanation:\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Log Odds Formula**\n",
        "The log odds (logit function) is defined as:\n",
        "$$\n",
        "\\ln\\left(\\frac{p}{1-p}\\right),\n",
        "$$\n",
        "where:\n",
        "- $p$ is the probability of success,\n",
        "- $1-p$ is the probability of failure.\n",
        "\n",
        "In logistic regression, we assume that the log odds is a linear function of the predictor variable $x$:\n",
        "$$\n",
        "\\ln\\left(\\frac{p}{1-p}\\right) = mx + b,\n",
        "$$\n",
        "where:\n",
        "- $m$ is the slope of the linear relationship,\n",
        "- $b$ is the intercept.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Solve for Odds**\n",
        "Exponentiate both sides to remove the logarithm:\n",
        "$$\n",
        "\\frac{p}{1-p} = e^{mx + b}.\n",
        "$$\n",
        "\n",
        "Here, $$ \\frac{p}{1-p} $$ represents the odds, and now we express it as an exponential function of $$ x $$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Solve for Probability $p$**\n",
        "Next, solve for $p$ (the probability of success). Start by isolating $p$:\n",
        "$$\n",
        "p = \\frac{\\text{Odds}}{1 + \\text{Odds}} = \\frac{e^{mx + b}}{1 + e^{mx + b}}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Logistic Regression Function**\n",
        "The resulting equation for probability is:\n",
        "$$\n",
        "p = \\frac{1}{1 + e^{-(mx + b)}}.\n",
        "$$\n",
        "\n",
        "This is the **logistic regression function**, which maps any linear combination of predictors (in this case, $$ mx + b $$) to a probability value between 0 and 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points**\n",
        "- The log odds ($\\ln(p/(1-p))$) are modeled as a linear function, here given by $mx + b$.\n",
        "- Solving for $p$ gives us the logistic regression equation:\n",
        "  $$\n",
        "  p = \\frac{1}{1 + e^{-(mx + b)}}.\n",
        "  $$\n",
        "- The sigmoid function ensures that probabilities stay within the range, making it ideal for classification problems.\n",
        "\n",
        "\n",
        "## Google Sheets\n",
        "All of this is manually calculated in [this google spreadsheet](https://docs.google.com/spreadsheets/d/1IVI6bVe33BRu9KdbJKwIw_k6IfD4_XjjBXfxNhBjiYw/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "5TXVl59lms1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoV7IO-Kir9Q",
        "outputId": "2ba4860f-c80b-4cac-8d66-1869583a1642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x[i]= 1 , actual value= 0 , predicted value= [0]\n",
            "x[i]= 2 , actual value= 0 , predicted value= [0]\n",
            "x[i]= 3 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 4 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 5 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 6 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 7 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 8 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 9 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 10 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 11 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 12 , actual value= 1 , predicted value= [1]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
        "x1 = x.reshape(x.size, 1)\n",
        "y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "reg = linear_model.LogisticRegression(fit_intercept=True)\n",
        "p = np.array([x1.size]).reshape(1, -1)\n",
        "reg.fit(x1, y)\n",
        "\n",
        "for i in range(0, x1.size):\n",
        "    p = np.array(x[i]).reshape(1, -1)\n",
        "    reg.predict(p)\n",
        "    print(\"x[i]=\", x[i], \", actual value=\", y[i], \", predicted value=\", reg.predict(p))\n"
      ]
    }
  ]
}