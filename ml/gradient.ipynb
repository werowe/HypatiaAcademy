{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHS8yL3bJEl8yK+oHcIEH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Excerpt From\n",
        "Deep Learning with Python, Third Edition\n",
        "François Chollet and Matthew Watson\n",
        "This material may be protected by copyright."
      ],
      "metadata": {
        "id": "LfS2vbg7Apfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "output = relu(malmul(input, W) + b\n",
        "\n",
        "where relu(x) = max(x, 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "xejy3qB_yaad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "f( x + ε_x ) = y + α * ε_x\n",
        "\n",
        "at some point p\n",
        "\n",
        "a = f'(p)\n",
        "\n",
        "if a < 0   ↑x ⇒   ↓f(x)\n",
        "\n",
        "if a > 0   ↑x ⇒   ↑f(x)\n",
        "\n",
        "|a| ↓ magnitude of Δ\n",
        "\n",
        "If you're trying to update x by a factor epsilon_x in order to minimize f(x), and you know the derivative of f, then your job is done”\n",
        "\n",
        "If you want to reduce the value of f(x), you just need to move x a little in the opposite direction from the derivative.\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "L0NlzfsNztga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The derivative of a tensor operation (or tensor function) is called a **gradient**.\n",
        "\n",
        "y_pred = matmul(W, x)\n",
        "\n",
        "loss_value = loss(y_pred, y_true)\n",
        "\n",
        "loss_value = f(W)\n",
        "\n",
        "\n",
        "f describes the curve (or high-dimensional surface) formed by loss values when W varies.\n",
        "\n",
        "\n",
        " current value is W0\n",
        "\n",
        " f'(W0) is tensor grad(loss_value, W0)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "yo4LS4xG2EEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the derivative of f in the point W0 is a tensor grad(loss_value, W0), with the same shape as W, where each coefficient grad(loss_value, W0)[i, j] indicates the direction and magnitude of the change in loss_value you observe when modifying W0[i, j]. That tensor grad(loss_value, W0) is the gradient of the function f(W) = loss_value in W0, also called “gradient of +loss_value+ with respect to W around W0\n",
        "\n",
        "\n",
        "\n",
        "> grad_ij is called the partial derivative of f with respect to W[i, j].\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "1UfBT3Kw3WjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the derivative of a function f(x) of a single coefficient can be interpreted as the slope of the curve of f.\n",
        "\n",
        "Likewise, grad(loss_value, W0) can be interpreted as the tensor describing the curvature of loss_value = f(W) around W0. Each partial derivative describes the curvature of f in a specific direction.\n",
        "\n",
        "with a function f(W) of a tensor, you can reduce loss_value = f(W) by moving W in the opposite direction from the gradient, such as an update of W1 = W0 - step * grad(f(W0), W0) where step is a small scaling factor. That means going against the curvature, which intuitively should put you lower on the curve.  \n",
        "\n",
        "A function's minimum is a point where the derivative is 0, so all you have to do is find all the points where the derivative goes to 0 and check for which of these points the function has the lowest value.\n"
      ],
      "metadata": {
        "id": "JEUmKbeR4J7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied to a neural network, that means finding analytically the combination of weight values that yields the smallest possible loss function. This can be done by solving the equation grad(f(W), W) = 0 for W.\n",
        "\n",
        "# mini-batch stochastic gradient descent (mini-batch SGD)\n",
        "\n",
        "1. Draw a batch of training samples x and corresponding targets y_true.  \n",
        "\n",
        "2. Run the model on x to obtain predictions y_pred (this is called the forward pass).  \n",
        "\n",
        "3. Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.  \n",
        "    \n",
        "4. Compute the gradient of the loss with regard to the model's parameters (this is called the backward pass).  \n",
        "    \n",
        "5. Move the parameters a little in the opposite direction from the gradient - for example W -= learning_rate * gradient - thus reducing the loss on the batch a bit.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "CXlIAXJE40kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turns any negative values to 0\n",
        "\n",
        "#Without activation functions like ReLU, neural networks would only be able to model linear relationships\n",
        "\n",
        "def relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()                             #2\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "            print(x[i, j])\n",
        "    return x\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "CIVaW3Yu693v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2,3],\n",
        "              [3,-4,3]])\n",
        "\n",
        "# x.ndim is equal to len(x.shape)\n",
        "\n",
        "assert len(x.shape) == 2"
      ],
      "metadata": {
        "id": "0ElGgXT67F7F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV3yUh4P8pmv",
        "outputId": "70d47cd0-78c8-432e-ee3b-b824682a5007"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = relu(x)\n",
        "\n",
        "x1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zi7YGWo9N6Y",
        "outputId": "cbc554cd-0cc7-4bce-bcf6-35fe42ddb506"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "3\n",
            "0\n",
            "3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [3, 0, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhzdMxJK87YC",
        "outputId": "3ee6d7f6-2946-4b7e-d65b-e6fe3880505c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}