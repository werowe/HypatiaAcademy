{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqkXzU06ymsItiz/LfuIdu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/ml/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokens and Masks\n",
        "\n",
        "In natural language processing (NLP), particularly when dealing with transformer-based models like BERT or GPT, the concept of an attention mask is crucial for handling variable-length input sequences. Here’s a detailed explanation of what an attention mask is and how it functions:\n",
        "\n",
        "### What is an Attention Mask?\n",
        "\n",
        "An attention mask is a binary tensor that indicates which elements in the input sequence should be attended to (considered) and which should not (ignored). This is particularly useful when you have input sequences of different lengths and you need to pad them to the same length for batch processing.\n",
        "\n",
        "### Why is it Needed?\n",
        "\n",
        "1. **Handling Padding**: When processing sequences of different lengths in batches, shorter sequences are often padded with a special token (e.g., `[PAD]`). These padding tokens should not contribute to the model's understanding of the sequence. The attention mask helps the model distinguish between real tokens and padding tokens.\n",
        "2. **Efficiency**: By ignoring the padded tokens, the model can focus its computational resources on the meaningful parts of the input, improving both efficiency and performance.\n",
        "\n",
        "### How Does it Work?\n",
        "\n",
        "- **Binary Masking**: The attention mask is typically a binary array (or tensor) where `1` indicates that the corresponding token should be attended to and `0` indicates that it should not.\n",
        "  - Example: For an input sequence `[The, quick, brown, fox, [PAD], [PAD]]`, the attention mask might be `[1, 1, 1, 1, 0, 0]`.\n",
        "\n",
        "### Implementation in Transformer Models\n",
        "\n",
        "When a transformer model processes an input sequence, it uses the attention mask in its attention mechanism. The attention mechanism computes attention scores, which determine how much focus each token should give to every other token in the sequence. The attention mask modifies these scores to ensure that the padded tokens are not considered.\n",
        "\n",
        "### Example with Code\n",
        "\n",
        "Here's a simple example using the Hugging Face Transformers library:\n",
        "\n",
        "```python\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Sample input sequences\n",
        "sentences = [\"The quick brown fox\", \"jumps over the lazy dog\"]\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the input sequences and pad them to the same length\n",
        "inputs = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# The inputs dictionary contains input_ids and attention_mask\n",
        "print(inputs['input_ids'])\n",
        "print(inputs['attention_mask'])\n",
        "\n",
        "# Input to the model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Extract the hidden states\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "```\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- **Input IDs**: Token IDs of the input sequence, padded where necessary.\n",
        "- **Attention Mask**: Binary mask indicating which tokens should be attended to.\n",
        "- **Model Processing**: The model uses the attention mask to ensure that padding tokens do not influence the processing of the input sequence.\n",
        "\n",
        "In summary, the attention mask is a fundamental tool in NLP for managing variable-length sequences and ensuring that padding tokens do not interfere with the learning process of the model."
      ],
      "metadata": {
        "id": "2EUcCOSYX0wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ah, I see the confusion. While the primary use of attention masks is to handle padding tokens, they can indeed be used to ignore certain words in a sentence. This can be particularly useful in tasks like selective attention in NLP models. Here’s a more detailed explanation and graphic illustrating this use case.\n",
        "\n",
        "### Selective Attention Mask\n",
        "\n",
        "In addition to padding, attention masks can be used to ignore specific tokens in the input sequence for various reasons, such as:\n",
        "\n",
        "- **Focus on Key Tokens**: To make the model focus on specific words or phrases.\n",
        "- **Exclusion of Stop Words**: To ignore common stop words that may not contribute significantly to the meaning of the sentence.\n",
        "\n",
        "### How it Works\n",
        "\n",
        "In this case, the attention mask will still be a binary tensor, but the `0`s will correspond to the tokens that should be ignored, even if they are not padding tokens.\n",
        "\n",
        "### Example\n",
        "\n",
        "Consider the sentence \"The quick brown fox jumps over the lazy dog.\" We might want to focus only on the key content words: \"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\".\n",
        "\n",
        "### Illustration\n",
        "\n",
        "Let's create a graphic to illustrate this selective attention mask concept.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "I will draw a graphic where specific words in a sentence are masked (ignored) using the attention mask.\n",
        "\n",
        "\n",
        "\n",
        "Here is a graphic illustrating the concept of a selective attention mask:\n",
        "\n",
        "- **Tokens**: The sequence of words in the sentence.\n",
        "- **Attention Mask**: A binary array indicating which words should be attended to (1) and which should be ignored (0).\n",
        "\n",
        "In this example, words like \"The\", \"over\", and \"the\" are marked with 0 in the attention mask, meaning they should be ignored. The remaining words are marked with 1, indicating they should be attended to by the model.\n",
        "\n",
        "This demonstrates how an attention mask can be used not only for handling padding but also for focusing on specific words in a sentence."
      ],
      "metadata": {
        "id": "EYNwC75RYWxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From video https://www.youtube.com/watch?v=QEaBAZQCtwE\n",
        "\n",
        "\n",
        "* how to use the pipeline how to use model\n",
        "\n",
        "* and tokenizer how to combine it with\n",
        "\n",
        "* pytorch or tensorflow how to save and\n",
        "\n",
        "* load models how to use models from the\n",
        "\n",
        "* official model hub and also how to fine\n",
        "\n",
        "* tune your own models\n",
        "\n",
        "\n",
        "works with tensorflow, pytorch, or flex\n"
      ],
      "metadata": {
        "id": "G9IKJ4xoBRno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "res = classifier(\"It's so hot today in Cyprus\")\n",
        "\n",
        "print(res)\n"
      ],
      "metadata": {
        "id": "OcFADEymvAez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ceea260-9e59-4d73-ba67-986c52ae76be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9993522763252258}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  apply tokenizer\n",
        "2.  feed preprocessed text to the model and applies model\n",
        "3.  post processor\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wMg0a08xAP3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation Pipeline"
      ],
      "metadata": {
        "id": "mq_U_QPvCCfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "res = generator(\n",
        "    \"Today we will learn about transformers\",\n",
        "    max_length=50,\n",
        "    num_return_sequences=2\n",
        "\n",
        ")\n",
        "\n",
        "for dis in res:\n",
        "  print(dis.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soAIWojKBrTR",
        "outputId": "27798082-3be3-403c-b08f-b8d3d49ba30f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values(['Today we will learn about transformers using techniques to achieve good results to optimize optimization. The following resources will be provided to you:'])\n",
            "dict_values(['Today we will learn about transformers and their technology.\\n\\n\\n\\nOur future is more than the invention of technology. As the field changes we need a revolution to see if changes can be turned into practical solutions to a problem, we want to'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "res = generator(\n",
        "    \"Cyprus is a boring place\",\n",
        "    candidate_labels=[\"criticism\", \"education\", \"business\"]\n",
        "\n",
        ")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(res['scores'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdVq9zVlDflT",
        "outputId": "a4e04110-8bab-43ba-93fc-434d0973872d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[0.9652251601219177, 0.028426004573702812, 0.006348819006234407]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification this means we can give it\n",
        "\n",
        "a text without knowing the corresponding\n",
        "\n",
        "label .  It then looks at the labels and sees which matches by percentage the text."
      ],
      "metadata": {
        "id": "8t1YVB9fD5_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "kS0J-0DhFFyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "\n",
        "model_name=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "text=\"It's so hot today in Cyprus.\"\n",
        "\n",
        "res = classifier(text)\n",
        "\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "res=tokenizer(text)\n",
        "print(\"\\n res=\", res)\n",
        "\n",
        "tokens=tokenizer.tokenize(text)\n",
        "print(\"\\n tokens=\", tokens)\n",
        "\n",
        "ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"\\n ids=\", ids)\n",
        "\n",
        "decoded_string=tokenizer.decode(ids)\n",
        "print(\"\\n decoded_string=\",decoded_string)\n",
        "\n",
        "\n",
        "# 101 is begin sentence\n",
        "# 102 is end sentence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNvihKenFHzD",
        "outputId": "b3740d1d-ed16-4cc0-8890-b5cd2116b50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9993836879730225}]\n",
            "\n",
            " res= {'input_ids': [101, 2009, 1005, 1055, 2061, 2980, 2651, 1999, 9719, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "\n",
            " tokens= ['it', \"'\", 's', 'so', 'hot', 'today', 'in', 'cyprus', '.']\n",
            "\n",
            " ids= [2009, 1005, 1055, 2061, 2980, 2651, 1999, 9719, 1012]\n",
            "\n",
            " decoded_string= it's so hot today in cyprus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# https://huggingface.co/ukr-models/xlm-roberta-base-uk\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ukr-models/xlm-roberta-base-uk\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"ukr-models/xlm-roberta-base-uk\")\n",
        "\n",
        "\n",
        "unmasker = pipeline('fill-mask', model='ukr-models/xlm-roberta-base-uk')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text = \"\"\"\n",
        "Ми знову запрошуємо підлітків  <mask> з України віком від 13 до 19 років на БЕЗКОШТОВНИЙ курс  у американського успішного викладача- програміста з програмування на Python. Python - це ТОП1 мова у світі програмування.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "res=tokenizer(text)\n",
        "print(\"\\n res=\", res)\n",
        "\n",
        "tokens=tokenizer.tokenize(text)\n",
        "print(\"\\n tokens=\", tokens)\n",
        "\n",
        "ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"\\n ids=\", ids)\n",
        "\n",
        "decoded_string=tokenizer.decode(ids)\n",
        "print(\"\\n decoded_string=\",decoded_string, \"\\n\\n\")\n",
        "\n",
        "unmasker(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8PSgZcVLbTV",
        "outputId": "fb386bec-35b5-49b0-e9d6-7146fc40bc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " res= {'input_ids': [0, 2688, 17222, 30223, 1228, 1618, 7749, 4234, 6, 31273, 210, 1702, 14213, 419, 1096, 702, 255, 953, 4664, 29, 24807, 7799, 2693, 28254, 14377, 4943, 84, 25902, 1041, 28385, 695, 18650, 15912, 9, 5725, 20730, 210, 5725, 2741, 29, 24420, 5, 24420, 20, 1544, 25947, 418, 8355, 84, 14042, 5725, 2741, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "\n",
            " tokens= ['▁Ми', '▁знову', '▁запрошує', 'мо', '▁під', 'літ', 'ків', '▁', '<mask>', '▁з', '▁України', '▁вік', 'ом', '▁від', '▁13', '▁до', '▁19', '▁років', '▁на', '▁БЕЗ', 'КО', 'Ш', 'ТОВ', 'НИЙ', '▁курс', '▁у', '▁американськ', 'ого', '▁успішно', 'го', '▁виклад', 'ача', '-', '▁програм', 'іста', '▁з', '▁програм', 'ування', '▁на', '▁Python', '.', '▁Python', '▁-', '▁це', '▁ТОП', '1', '▁мова', '▁у', '▁світі', '▁програм', 'ування', '.']\n",
            "\n",
            " ids= [2688, 17222, 30223, 1228, 1618, 7749, 4234, 6, 31273, 210, 1702, 14213, 419, 1096, 702, 255, 953, 4664, 29, 24807, 7799, 2693, 28254, 14377, 4943, 84, 25902, 1041, 28385, 695, 18650, 15912, 9, 5725, 20730, 210, 5725, 2741, 29, 24420, 5, 24420, 20, 1544, 25947, 418, 8355, 84, 14042, 5725, 2741, 5]\n",
            "\n",
            " decoded_string= Ми знову запрошуємо підлітків <mask> з України віком від 13 до 19 років на БЕЗКОШТОВНИЙ курс у американського успішного викладача- програміста з програмування на Python. Python - це ТОП1 мова у світі програмування. \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5255579352378845,\n",
              "  'token': 23347,\n",
              "  'token_str': 'ІТ',\n",
              "  'sequence': 'Ми знову запрошуємо підлітків ІТ з України віком від 13 до 19 років на БЕЗКОШТОВНИЙ курс у американського успішного викладача- програміста з програмування на Python. Python - це ТОП1 мова у світі програмування.'},\n",
              " {'score': 0.13842427730560303,\n",
              "  'token': 29189,\n",
              "  'token_str': 'службовців',\n",
              "  'sequence': 'Ми знову запрошуємо підлітків службовців з України віком від 13 до 19 років на БЕЗКОШТОВНИЙ курс у американського успішного викладача- програміста з програмування на Python. Python - це ТОП1 мова у світі програмування.'},\n",
              " {'score': 0.059892717748880386,\n",
              "  'token': 4,\n",
              "  'token_str': ',',\n",
              "  'sequence': 'Ми знову запрошуємо підлітків, з України віком від 13 до 19 років на БЕЗКОШТОВНИЙ курс у американського успішного викладача- програміста з програмування на Python. Python - це ТОП1 мова у світі програмування.'},\n",
              " {'score': 0.04573393613100052,\n",
              "  'token': 23348,\n",
              "  'token_str': 'спеціаліст',\n",
              "  'sequence': 'Ми знову запрошуємо підлітків спеціаліст з України віком від 13 до 19 років на БЕЗКОШТОВНИЙ курс у американського успішного викладача- програміста з програмування на Python. Python - це ТОП1 мова у світі програмування.'},\n",
              " {'score': 0.03969251736998558,\n",
              "  'token': 18302,\n",
              "  'token_str': 'STEM',\n",
              "  'sequence': 'Ми знову запрошуємо підлітків STEM з України віком від 13 до 19 років на БЕЗКОШТОВНИЙ курс у американського успішного викладача- програміста з програмування на Python. Python - це ТОП1 мова у світі програмування.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Recognition\n"
      ],
      "metadata": {
        "id": "OsAH_FvPJKFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "captioner = pipeline(model=\"ydshieh/vit-gpt2-coco-en\")\n",
        "captioner(\"https://th.bing.com/th/id/OIP.SBYtWe52Cb3ecG65Z0ae8wAAAA?rs=1&pid=ImgDetMain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21eiKLIWJOG9",
        "outputId": "a3bd99d8-288d-48f3-af84-aee8242c02f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'a large boat with a large cargo ship on it '}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}