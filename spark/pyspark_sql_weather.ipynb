{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgCd6iPytygVomSEsG7B2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/spark/pyspark_sql_weather.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# How to use Apache Spark with Python\n",
        "\n",
        "\n",
        "Here we show how to use Spark SQL to query data using SQL in Apache Spark.  \n",
        "\n",
        "PySpark can be installed as a Python package using `pip install pyspark` but that's not necessary with Google colab as it's already included.\n",
        "\n",
        "## Program Steps\n",
        "\n",
        "1.  Download sample csv data.\n",
        "2.  Read the data into a Spark Dataframe\n",
        "3.  Create a Spark View so we can run SQL queries\n",
        "4.  Write and run some SQL queries\n"
      ],
      "metadata": {
        "id": "aRa0vVJnr36-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Data\n",
        "\n",
        "First download weather data in the bash shell\n",
        "\n",
        "> Note: Add the **Addenum** below is you want to see how this data was create.  "
      ],
      "metadata": {
        "id": "mxCL3k-wpZE8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcENJEoaok_P",
        "outputId": "938c44a5-2cc2-4abe-bbd5-f32d1db52aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-26 06:35:22--  https://raw.githubusercontent.com/werowe/HypatiaAcademy/refs/heads/master/basics/combined_paphos2024.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647674 (1.6M) [text/plain]\n",
            "Saving to: ‘combined_paphos2024.csv.2’\n",
            "\n",
            "combined_paphos2024 100%[===================>]   1.57M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-12-26 06:35:22 (33.4 MB/s) - ‘combined_paphos2024.csv.2’ saved [1647674/1647674]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/werowe/HypatiaAcademy/refs/heads/master/basics/combined_paphos2024.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the csv file into a Spark dataframe\n",
        "\n",
        "pip install pyspark is not necessary since Google Colab has already installed the pyspark packages\n",
        "\n",
        "Here we create a Spark dataframe reading the data we have loaded to the load drive in the Colab virtual machine.\n",
        "\n"
      ],
      "metadata": {
        "id": "DPz4oXmvpgGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"weather\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\n",
        "    \"combined_paphos2024.csv\",\n",
        "    header=True,        # Use the first row as column names\n",
        "    inferSchema=True,   # Automatically infer data types\n",
        "    sep=\",\",            # Specify delimiter (default is ',')\n",
        "    encoding=\"UTF-8\"    # Handle encoding\n",
        ")"
      ],
      "metadata": {
        "id": "GzVEHinGollw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we show the columns so we can see what data we have downloaded.\n",
        "\n"
      ],
      "metadata": {
        "id": "SLCN2xXUpyrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7gMhYIlo0cW",
        "outputId": "73b61b56-fc0a-40df-f5b2-3b31c496107a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['name',\n",
              " 'datetime',\n",
              " 'temp',\n",
              " 'feelslike',\n",
              " 'dew',\n",
              " 'humidity',\n",
              " 'precip',\n",
              " 'precipprob',\n",
              " 'preciptype',\n",
              " 'snow',\n",
              " 'snowdepth',\n",
              " 'windgust',\n",
              " 'windspeed',\n",
              " 'winddir',\n",
              " 'sealevelpressure',\n",
              " 'cloudcover',\n",
              " 'visibility',\n",
              " 'solarradiation',\n",
              " 'solarenergy',\n",
              " 'uvindex',\n",
              " 'severerisk',\n",
              " 'conditions',\n",
              " 'icon',\n",
              " 'stations']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create a temporary view in Apache Spark so that we can run queries again it."
      ],
      "metadata": {
        "id": "R_1gobtfqzXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.createOrReplaceTempView(\"weather\")"
      ],
      "metadata": {
        "id": "EEEfDj6OotxB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write SQL Queries\n",
        "\n",
        "Now is is just a matter on create SQL queries and then runing them."
      ],
      "metadata": {
        "id": "Np-G7Iz5q6YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql = '''\n",
        "SELECT SUM(precip) AS total_precip, MONTH(datetime) AS month\n",
        "FROM weather\n",
        "GROUP BY MONTH(datetime)\n",
        "\n",
        "'''\n",
        "\n",
        "result = spark.sql(sql)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRvigNV-owmE",
        "outputId": "5059ec18-ef42-4f89-f5fe-e96344916dff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|      total_precip|month|\n",
            "+------------------+-----+\n",
            "|205.63699999999983|   12|\n",
            "|1.6430000000000005|    9|\n",
            "|             0.008|    8|\n",
            "|               0.0|    7|\n",
            "|1.5000000000000002|   10|\n",
            "|166.32799999999978|   11|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql = '''\n",
        "select max(temp) from weather  WHERE MONTH(datetime) = 8\n",
        "'''\n",
        "\n",
        "result = spark.sql(sql)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vc_NrParPO5",
        "outputId": "5d9f75df-edb8-4106-b1bc-8b2c43e6cad5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|max(temp)|\n",
            "+---------+\n",
            "|     33.1|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql = '''\n",
        "SELECT max(temp) AS max_temp, MONTH(datetime) AS month\n",
        "FROM weather\n",
        "GROUP BY MONTH(datetime)\n",
        "'''\n",
        "\n",
        "result = spark.sql(sql)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yg3H8V5rJw1",
        "outputId": "f8212bad-03c2-48de-ce06-b4cc913e2af4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|max_temp|month|\n",
            "+--------+-----+\n",
            "|    22.6|   12|\n",
            "|    32.1|    9|\n",
            "|    33.1|    8|\n",
            "|    34.6|    7|\n",
            "|    28.8|   10|\n",
            "|    27.0|   11|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Addendum: How this Data was created\n",
        "\n",
        "This data was downloaded using the API https://www.visualcrossing.com/weather/weather-data-services#\n",
        "\n",
        "The code to do that is [here](https://github.com/werowe/HypatiaAcademy/blob/master/stats/paphos_daily_weather_csv.ipynb).  And the code to combine multiple days of data into one csv file is [here](https://github.com/werowe/HypatiaAcademy/blob/master/stats/consolidate.ipynb).\n"
      ],
      "metadata": {
        "id": "bgltcJiJqG9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Reading\n",
        "\n",
        "Here's an example of [how to do logistic regression using PySpark](https://github.com/werowe/HypatiaAcademy/blob/841c6dbd22daa9dba58fc629cfb3b5135b837cd6/spark/sparkLR.ipynb#L4)"
      ],
      "metadata": {
        "id": "IZJ9JYCRszZM"
      }
    }
  ]
}