{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODUN3rr09aa+AM5+Jso8up",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werowe/HypatiaAcademy/blob/master/m/logistic_regression_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the linear function in logistic regression is expressed as $$ mx + b $$ (where $m$ is the slope and $b$ is the intercept), we can derive the **logistic regression function** step by step. Here's the explanation:\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Log Odds Formula**\n",
        "The log odds (logit function) is defined as:\n",
        "$$\n",
        "\\ln\\left(\\frac{p}{1-p}\\right),\n",
        "$$\n",
        "where:\n",
        "- $p$ is the probability of success,\n",
        "- $1-p$ is the probability of failure.\n",
        "\n",
        "In logistic regression, we assume that the log odds is a linear function of the predictor variable $x$:\n",
        "$$\n",
        "\\ln\\left(\\frac{p}{1-p}\\right) = mx + b,\n",
        "$$\n",
        "where:\n",
        "- $m$ is the slope of the linear relationship,\n",
        "- $b$ is the intercept.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Solve for Odds**\n",
        "Exponentiate both sides to remove the logarithm:\n",
        "$$\n",
        "\\frac{p}{1-p} = e^{mx + b}.\n",
        "$$\n",
        "\n",
        "Here, $$ \\frac{p}{1-p} $$ represents the odds, and now we express it as an exponential function of $$ x $$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Solve for Probability $$ p $$**\n",
        "Next, solve for $p$ (the probability of success). Start by isolating $p$:\n",
        "$$\n",
        "p = \\frac{\\text{Odds}}{1 + \\text{Odds}} = \\frac{e^{mx + b}}{1 + e^{mx + b}}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Logistic Regression Function**\n",
        "The resulting equation for probability is:\n",
        "$$\n",
        "p = \\frac{1}{1 + e^{-(mx + b)}}.\n",
        "$$\n",
        "\n",
        "This is the **logistic regression function**, which maps any linear combination of predictors (in this case, $$ mx + b $$) to a probability value between 0 and 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points**\n",
        "- The log odds ($\\ln(p/(1-p))$) are modeled as a linear function, here given by $mx + b$.\n",
        "- Solving for $p$ gives us the logistic regression equation:\n",
        "  $$\n",
        "  p = \\frac{1}{1 + e^{-(mx + b)}}.\n",
        "  $$\n",
        "- The sigmoid function ensures that probabilities stay within the range, making it ideal for classification problems.\n"
      ],
      "metadata": {
        "id": "5TXVl59lms1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoV7IO-Kir9Q",
        "outputId": "2ba4860f-c80b-4cac-8d66-1869583a1642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x[i]= 1 , actual value= 0 , predicted value= [0]\n",
            "x[i]= 2 , actual value= 0 , predicted value= [0]\n",
            "x[i]= 3 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 4 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 5 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 6 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 7 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 8 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 9 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 10 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 11 , actual value= 1 , predicted value= [1]\n",
            "x[i]= 12 , actual value= 1 , predicted value= [1]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
        "x1 = x.reshape(x.size, 1)\n",
        "y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "reg = linear_model.LogisticRegression(fit_intercept=True)\n",
        "p = np.array([x1.size]).reshape(1, -1)\n",
        "reg.fit(x1, y)\n",
        "\n",
        "for i in range(0, x1.size):\n",
        "    p = np.array(x[i]).reshape(1, -1)\n",
        "    reg.predict(p)\n",
        "    print(\"x[i]=\", x[i], \", actual value=\", y[i], \", predicted value=\", reg.predict(p))\n"
      ]
    }
  ]
}