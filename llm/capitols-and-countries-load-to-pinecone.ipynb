{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a74b62b",
   "metadata": {},
   "source": [
    "# Create Index and Import Data\n",
    "\n",
    "Other notebooks:\n",
    "\n",
    "* [query data](http://localhost:9999/notebooks/Documents/langchain/pinecode-read-and-write-data.ipynb)\n",
    "\n",
    "A **transformer model** generates sentence embeddings by encoding a text sequence into dense numeric vectors that represent its meaning in a multidimensional space. These vectors capture semantic information such that texts with similar meanings are close together in that space.\n",
    "\n",
    "Transformer models process input text through multiple layers of **self-attention** and **feed-forward neural networks**, enabling them to consider the relationships between all words in a sentence simultaneously. Unlike earlier models (like Word2Vec or GloVe) that treat words independently, transformers analyze each token in context—looking both forward and backward—so that the embedding for a word like \"bank\" differs depending on whether it appears in “river bank” or “bank account”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6347818",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "YOUR_API_KEY=\"pcsk_4Sb7ji_4et5DjhU46G1avXaUMjtZru5XQvPKqcR7gnXC6VcVCvzFLcTRqVjLh2N7oxZqrf\"\n",
    "\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=YOUR_API_KEY)\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"capitols-and-countries-2\",\n",
    "    vector_type=\"dense\",\n",
    "    dimension=384,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    deletion_protection=\"disabled\",\n",
    "    tags={\"environment\": \"development\"}\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c12179e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(\"word-test.v1.txt\") as f:\n",
    "    for line in f:\n",
    "        # Skip comments and empty lines\n",
    "        if not line.strip() or line.startswith(\"//\") or line.startswith(\":\"):\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        # You can combine pairs or treat each word separately\n",
    "        lines.append(\" \".join(parts))  # e.g., 'Athens Greece Baghdad Iraq'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10991d6",
   "metadata": {},
   "source": [
    "Here, the model handles tokenization, attention processing, and mean pooling automatically, resulting in 384-dimensional embeddings that capture the semantic essence of each sentence.\n",
    "\n",
    "**Mean pooling**: Models like Sentence Transformers (e.g., all-MiniLM-L6-v2) compute the mean of all token embeddings, weighted by the attention mask, to produce a compact embedding that reflects the overall semantic content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "16528e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: langchain/bin/activate: No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!source langchain/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7759b87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pcsk_4Sb7ji_4et5DjhU46G1avXaUMjtZru5XQvPKqcR7gnXC6VcVCvzFLcTRqVjLh2N7oxZqrf'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()   \n",
    "YOUR_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "YOUR_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c32e2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Local, fast, decent for general text\n",
    "embeddings = model.encode(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "57cddd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dcd89427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    " \n",
    "# Build Pinecone-ready DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"id\" : [f\"vec_{i}\" for i in range(len(lines))],\n",
    "    \"values\" : [np.array(x, dtype=np.float32).tolist() for x in embeddings],\n",
    "    \"metadata\": [json.dumps({\"text\": line}) for line in lines]\n",
    "})\n",
    "\n",
    "# Convert each embedding to numpy array for compatibility\n",
    "#df[\"embedding\"] = df[\"embedding\"].apply(np.array)\n",
    "df.to_parquet(\"embeddings.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "#aws s3 cp embeddings.parquet s3://capitols-and-countries/imports/__default__/0.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4182ebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19544 entries, 0 to 19543\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        19544 non-null  object\n",
      " 1   values    19544 non-null  object\n",
      " 2   metadata  19544 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 458.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ea293af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./embeddings.parquet to s3://capitols-and-countries/imports/__default__/0.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp embeddings.parquet s3://capitols-and-countries/imports/__default__/0.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "126efe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"id\": \"15\"\n",
       "}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ImportErrorMode\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=YOUR_API_KEY)\n",
    "\n",
    "# Connect to your index\n",
    "index = pc.Index(\"capitols-and-countries-2\")\n",
    "\n",
    " \n",
    "\n",
    "index.delete_namespace(namespace=\"__default__\")\n",
    "\n",
    "\n",
    "# Define your S3 Parquet path\n",
    "s3_uri = \"s3://capitols-and-countries/imports\"\n",
    "\n",
    "# Start import\n",
    "index.start_import(\n",
    "    uri=s3_uri,\n",
    "    error_mode=ImportErrorMode.CONTINUE,  # or ABORT to stop on first error\n",
    "    integration_id=\"ac88a7fa-1c63-43fe-821e-5fac2deac81c\"  # omit if public S3 file\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "17fbcc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"id\": \"15\",\n",
       "    \"uri\": \"s3://capitols-and-countries/imports\",\n",
       "    \"status\": \"Completed\",\n",
       "    \"percent_complete\": 100.0,\n",
       "    \"records_imported\": 19544,\n",
       "    \"created_at\": \"2025-10-19T07:10:14.248853+00:00\",\n",
       "    \"finished_at\": \"2025-10-19T07:11:10.454674+00:00\"\n",
       "}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_import(id=\"15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3fb3781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 19544}},\n",
      " 'total_vector_count': 19544,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "stats = index.describe_index_stats()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "10a4aff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: string\n",
      "values: list<element: double>\n",
      "  child 0, element: double\n",
      "metadata: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 601\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pq.read_table(\"embeddings.parquet\")\n",
    "print(table.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de784afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
